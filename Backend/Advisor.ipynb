{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Forzar a que use la GPU de NVIDIA si hay dudas\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "print(f\"¬øCuda disponible?: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Entrenando en: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # Si sale False, dinos qu√© sale aqu√≠:\n",
    "    print(f\"Versi√≥n detectada: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"klmVEuTiYxKVkKRwtyGF\")\n",
    "project = rf.workspace(\"samuel-gonzalez-duran-l6jsa\").project(\"fashion-detector-n3bi3\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov11\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba228a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "results = model.train(\n",
    "    data=\"C:/Users/andsa/Documents/202526ULPGC/VC/trabajofinal/ForecastWear/Backend/Fashion-Detector-3/data.yaml\",\n",
    "    epochs = 50,\n",
    "    imgsz=640,\n",
    "    device=0,\n",
    "    batch=4,    \n",
    "    workers=4,   \n",
    "    plots=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "def detect_person(image):\n",
    "    model = YOLO('yolo11n.pt') \n",
    "    imagen = cv2.imread(image)\n",
    "    results = model(imagen)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  \n",
    "    scores = results[0].boxes.conf.cpu().numpy()  \n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()  \n",
    "    class_names = model.names \n",
    "    crops_person = []\n",
    "    for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "        class_id = int(class_id)\n",
    "        if class_id == 0 and score >= 0.5: \n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            crops_person.append([x1, y1, x2, y2])\n",
    "            label = f\"{class_names[int(class_id)]}: {score:.2f}\"\n",
    "            cv2.rectangle(imagen, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(imagen, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)    \n",
    "    if len(crops_person) == 0:\n",
    "        print(\"No se detect√≥ ninguna persona en la imagen.\")\n",
    "        return None\n",
    "    else:\n",
    "        person_croped = croped_person(crops_person[0],imagen)\n",
    "        cv2.imwrite(\"funciona1.jpg\",person_croped)\n",
    "        person_croped_image = \"funciona1.jpg\"\n",
    "        mujer_cholas = detect_clothes(person_croped_image)\n",
    "        print(mujer_cholas[1])\n",
    "        recorte_shoes = croped_person(mujer_cholas[1][0],mujer_cholas[0])\n",
    "        cv2.imshow('Detectada',mujer_cholas[0])\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "\n",
    "def croped_person(coordenates,imagen_rgb):\n",
    "    return imagen_rgb[coordenates[1]:coordenates[3], coordenates[0]:coordenates[2]]\n",
    "\n",
    "def detect_clothes(person):\n",
    "    model = YOLO(\"C:/Users/andsa/Documents/202526ULPGC/VC/trabajofinal/ForecastWear/Backend/runs/detect/train2/weights/best.pt\")\n",
    "    image = cv2.imread(person)\n",
    "    results = model(image)[0]\n",
    "    class_names = model.names\n",
    "    shoes_map = []\n",
    "    for box in results.boxes:\n",
    "        score = box.conf[0].item()\n",
    "        class_id = int(box.cls[0].item())\n",
    "        if score >= 0.5:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            if class_names[class_id] == \"shoe\":\n",
    "                shoes_map.append([x1, y1, x2, y2])\n",
    "            label = f\"{class_names[class_id]}: {score:.2f}\"\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(image, label, (x1, y1 - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "\n",
    "    return [image,shoes_map]\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_person('cholas1.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095dc70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "c:\\Users\\andsa\\anaconda3\\envs\\shoeclasificator\\lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://92abed60c14c6a0093.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://92abed60c14c6a0093.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import  SegformerForSemanticSegmentation\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "segment_processor = AutoImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "segment_model = SegformerForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "color_clothes = {\n",
    "    0:  (0, 0, 0),        # Background - negro\n",
    "    1:  (255, 105, 180),  # Hat - rosa\n",
    "    3:  (0, 255, 255),    # Sunglasses - cian\n",
    "    4:  (255, 0, 0),      # Upper-clothes - rojo\n",
    "    5:  (0, 255, 0),      # Skirt - verde\n",
    "    6:  (0, 0, 255),      # Pants - azul\n",
    "    7:  (255, 255, 0),    # Dress - amarillo\n",
    "    8:  (255, 165, 0),    # Belt - naranja\n",
    "    9:  (255, 0, 255),    # Left-shoe - fucsia\n",
    "    10: (128, 0, 128),    # Right-shoe - morado\n",
    "    16: (0, 128, 128),    # Bag - verde azulado\n",
    "    17: (128, 128, 128),  # Scarf - gris\n",
    "}\n",
    "classes_to_classify = {\n",
    "    4: \"Upper-clothes\",\n",
    "    5: \"Skirt\",\n",
    "    6: \"Pants\",\n",
    "    7: \"Dress\",\n",
    "    9: \"Left-shoe\",\n",
    "    10: \"Right-shoe\",\n",
    "}\n",
    "\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    h, w = mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for cls, color in color_clothes.items():\n",
    "        color_mask[mask == cls] = color\n",
    "\n",
    "    return color_mask\n",
    "\n",
    "def overlay_mask(image, mask, alpha=0.5):\n",
    "    image_np = np.array(image)\n",
    "    color_mask = colorize_mask(mask)\n",
    "\n",
    "    overlay = image_np.copy()\n",
    "    overlay[color_mask.sum(axis=2) > 0] = (\n",
    "        (1 - alpha) * image_np[color_mask.sum(axis=2) > 0] +\n",
    "        alpha * color_mask[color_mask.sum(axis=2) > 0]\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def segment_clothing(image):\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    image_pil = Image.fromarray(image).convert(\"RGB\")\n",
    "\n",
    "    inputs = segment_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = segment_model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image_pil.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "    overlay = overlay_mask(image_pil, mask)\n",
    "\n",
    "    return overlay\n",
    "\n",
    "def extract_class(image, mask, class_id):\n",
    "    \"\"\"\n",
    "    Devuelve una imagen donde solo la clase class_id aparece (el resto negro)\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    class_mask = (mask == class_id).astype(np.uint8)\n",
    "\n",
    "    if class_mask.sum() == 0:\n",
    "        return None  # No se detect√≥ esta prenda\n",
    "\n",
    "    class_mask_3ch = np.stack([class_mask]*3, axis=-1)\n",
    "    segmented = image_np * class_mask_3ch\n",
    "    return Image.fromarray(segmented)\n",
    "def segment_and_classify_by_type(image):\n",
    "    if image is None:\n",
    "        return {}, None\n",
    "\n",
    "    image_pil = Image.fromarray(image).convert(\"RGB\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Segmentaci√≥n\n",
    "    inputs = segment_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = segment_model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image_pil.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Imagen overlay para mostrar\n",
    "    overlay = overlay_mask(image_pil, mask).astype(np.uint8)\n",
    "\n",
    "    # 2Ô∏è‚É£ Clasificaci√≥n por clase\n",
    "    results = {}\n",
    "    for class_id, class_name in classes_to_classify.items():\n",
    "        class_img = extract_class(image_pil, mask, class_id)\n",
    "        if class_img is None:\n",
    "            continue\n",
    "\n",
    "        # Clasificaci√≥n ViT\n",
    "        inputs_vit = processor(images=class_img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs_vit = model(**inputs_vit)\n",
    "            logits_vit = outputs_vit.logits\n",
    "            probs = torch.nn.functional.softmax(logits_vit, dim=1).squeeze()\n",
    "            top_prob, top_idx = torch.topk(probs, 1)\n",
    "            predicted_label = model.config.id2label[top_idx.item()]\n",
    "        \n",
    "        results[class_name] = predicted_label\n",
    "\n",
    "    return overlay, results\n",
    "\n",
    "\n",
    "def classify_image(image):\n",
    "    if image is None: return None\n",
    "    image = Image.fromarray(image).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).squeeze()\n",
    "        top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    return {model.config.id2label[idx.item()]: round(prob.item(), 3) for prob, idx in zip(top_probs, top_indices)}\n",
    "\n",
    "css =\"\"\n",
    "with gr.Blocks(theme=gr.themes.Soft(),css=css) as demo:\n",
    "    gr.Markdown(\"# üå¶Ô∏è Asesor del Clima\")\n",
    "    \n",
    "    with gr.Column():\n",
    "        input_img = gr.Image(type=\"numpy\", label=\"Entrada de Imagen\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            gr.Column(scale=2) \n",
    "            btn = gr.Button(\"Analizar Outfit\", variant=\"primary\", scale=2) \n",
    "            gr.Column(scale=2) \n",
    "        \n",
    "    output_img = gr.Image(label=\"Ropa detectada\")\n",
    "    output_label = gr.Textbox(label=\"Predicci√≥n por tipo de prenda\")\n",
    "\n",
    "    btn.click(\n",
    "        fn=segment_and_classify_by_type,\n",
    "        inputs=input_img,\n",
    "        outputs=[output_img, output_label]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        demo.launch(share=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoeclasificator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
